{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful documentation links\n",
    "\n",
    "- https://python.langchain.com/docs/get_started/introduction.html\n",
    "- https://python.langchain.com/docs/use_cases/question_answering/\n",
    "- https://github.com/langchain-ai/langchain\n",
    "- https://github.com/langchain-ai/chat-langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in Secret key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "# key using env\n",
    "os.environ['openaiapikey']='sk-keyyyyy'\n",
    "openai.api_key = os.getenv('openaiapikey')\n",
    "\n",
    "#key using txt\n",
    "f = open('C:\\\\Users\\\\Marcial\\\\Desktop\\\\desktop_openai.txt')\n",
    "api_key = f.read()\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat messages\n",
    "- SystemMessage: the persona\n",
    "- HumanMessage: a msg from the user pov\n",
    "- AIMessage: the response to the human message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.schema import (AIMessage, HumanMessage, SystemMessage)\n",
    "\n",
    "result = chat([SystemMessage(content='You are a very rude teenager'),\n",
    "               HumanMessage(content='Can you tell me a fact about Earth?')]\n",
    "               temperature = 0.1,  #lower = more focused and deterministic output, 0 to 2\n",
    "               presence_penalty = 1, #penalise tokens that appear many times\n",
    "               max_tokens = 100)\n",
    "\n",
    "result.content #outputs an AI message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (ChatPromptTemplate, PromptTemplate, \n",
    "    SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "\n",
    "\n",
    "def travel_idea(interest,budget):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        interest: A str interest or hobby (e.g. fishing)\n",
    "        budget: A str budget (e.g. $10,000)\n",
    "    '''\n",
    "    # PART ONE: SYSTEM\n",
    "    system_template=\"You are an AI Travel Agent that helps people plan trips about {interest} on a budget of {budget}\"\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    \n",
    "    # PART TWO: HUMAN REQUEST\n",
    "    human_template=\"{travel_help_request}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "    \n",
    "    # PART THREE: COMPILE TO CHAT\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, \n",
    "                                                    human_message_prompt])\n",
    "    \n",
    "    # PART FOUR: INSERT VARIABLES\n",
    "    request = chat_prompt.format_prompt(interest=interest, \\\n",
    "                                        budget=budget, \\\n",
    "                                        travel_help_request=\"Please give me an example travel itinerary\").to_messages()\n",
    "    \n",
    "    # PART FIVE: CHAT REQUEST\n",
    "    chat = ChatOpenAI(openai_api_key=api_key, \\\n",
    "                      temperature=0, \\\n",
    "                      presence_penalty=0.5, \\\n",
    "                      max_tokens=100)\n",
    "    result = chat(request)\n",
    "    \n",
    "    return result.content\n",
    "\n",
    "#use input functions\n",
    "activity_input = input(\"Enter an activity for your travel idea: \")\n",
    "budget_input = input(\"Enter your budget for the trip: \")\n",
    "\n",
    "print(activity_input)\n",
    "print(budget_input)\n",
    "print(travel_idea(activity_input,budget_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.predict(\"Tell me a fact about Mars\")\n",
    "\n",
    "# You will notice this reply is instant!\n",
    "# does not ping the api! \n",
    "llm.predict('Tell me a fact about Mars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving prompt ro Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"Question: {question}\\n\\nAnswer: Let's think step by step.\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "prompt.save(\"prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All prompts are loaded through the `load_prompt` function.\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "loaded_prompt = load_prompt('prompt.json')\n",
    "loaded_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing output\n",
    "date parsing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=api_key)\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt to compliment output parser\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\"You always reply to questions only in datetime patterns.\")\n",
    "template_text = \"{request}\\n{format_instructions}\"\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(template_text)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt,human_prompt])\n",
    "\n",
    "request = chat_prompt.format_prompt(request=\"What date was the 13th Amendment ratified in the US?\",\n",
    "                   format_instructions=output_parser.get_format_instructions()\n",
    "                   ).to_messages()\n",
    "\n",
    "result = model(request,temperature=0)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix parsing issue: aut-fix parser\n",
    "#consider system prompt template in combination\n",
    "\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "# output_parser = DatetimeOutputParser()\n",
    "misformatted = result.content\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=output_parser, llm=model)\n",
    "new_parser.parse(misformatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few shot prompt: learning from example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing\n",
    "chat = ChatOpenAI(openai_api_key=api_key, temperature=0)\n",
    "\n",
    "#example of a legal text\n",
    "legal_text = \"The provisions herein shall be severable, and if any provision or portion thereof is deemed invalid, illegal, or unenforceable by a court of competent jurisdiction, the remaining provisions or portions thereof shall remain in full force and effect to the maximum extent permitted by law.\"\n",
    "example_input_one = HumanMessagePromptTemplate.from_template(legal_text)\n",
    "\n",
    "#example of the simplified (plain) text\n",
    "plain_text = \"The rules in this agreement can be separated. If a court decides that one rule or part of it is not valid, illegal, or cannot be enforced, the other rules will still apply and be enforced as much as they can under the law.\"\n",
    "example_output_one = AIMessagePromptTemplate.from_template(plain_text)\n",
    "\n",
    "\n",
    "#set up human prompt template\n",
    "human_template = \"{legal_text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, \n",
    "     example_input_one, \n",
    "     example_output_one, \n",
    "     human_message_prompt])\n",
    "\n",
    "#example input\n",
    "some_example_text = \"The grantor, being the fee simple owner of the real property herein described, conveys and warrants to the grantee, his heirs and assigns, all of the grantor's right, title, and interest in and to the said property, subject to all existing encumbrances, liens, and easements, as recorded in the official records of the county, and any applicable covenants, conditions, and restrictions affecting the property, in consideration of the sum of [purchase price] paid by the grantee.\"\n",
    "request = chat_prompt.format_prompt(legal_text=some_example_text).to_messages()\n",
    "result = chat(request)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to document and retrieve relavent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor \n",
    "\n",
    "import os\n",
    "f = open('C:\\\\Users\\\\Marcial\\\\Desktop\\\\desktop_openai.txt')\n",
    "os.environ['OPENAI_API_KEY'] = f.read()\n",
    "\n",
    "\n",
    "def us_constitution_helper(question):\n",
    "    '''\n",
    "    Takes in a question about the US Constitution and returns the most relevant\n",
    "    part of the constitution. Notice it may not directly answer the actual question!\n",
    "    \n",
    "    Follow the steps below to fill out this function:\n",
    "    '''\n",
    "    # PART ONE:\n",
    "    # LOAD \"some_data/US_Constitution in a Document object\n",
    "    loader = TextLoader(\"some_data/US_Constitution.txt\")\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # PART TWO\n",
    "    # Split the document into chunks (you choose how and what size)\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=500)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # PART THREE\n",
    "    # EMBED THE Documents (now in chunks) to a persisted ChromaDB\n",
    "    # embedding_function = OpenAIEmbeddings()\n",
    "    embedding_function = OpenAIEmbeddings()\n",
    "    db = Chroma.from_documents(docs, embedding_function, persist_directory='./US_Constitution')\n",
    "    db.persist()\n",
    "\n",
    "    # PART FOUR\n",
    "    # Use ChatOpenAI and ContextualCompressionRetriever to return the most\n",
    "    # relevant part of the documents.\n",
    "\n",
    "    # results = db.similarity_search(\"What is the 13th Amendment?\")\n",
    "    # print(results[0].page_content) # NEED TO COMPRESS THESE RESULTS!\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "    compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, \n",
    "                                                           base_retriever=db.as_retriever())\n",
    "\n",
    "    compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "\n",
    "    return compressed_docs[0].page_content\n",
    "\n",
    "print(us_constitution_helper(\"What is the 13th Amendment?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to SQL\n",
    "- https://python.langchain.com/docs/use_cases/tabular/\n",
    "- https://python.langchain.com/docs/use_cases/tabular/sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "\n",
    "# For data-sensitive projects, you can specify return_direct=True in the SQLDatabaseChain initialization to directly return the output of the SQL query without any additional formatting.\n",
    "db = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\n",
    "llm = OpenAI(temperature=0, verbose=True)\n",
    "\n",
    "\n",
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
    "db_chain.run(\"How many employees are there?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the Language Model generates invalid SQL \n",
    "with small mistakes that can be \n",
    "self-corrected using the same technique \n",
    "used by the SQL Database Agent \n",
    "to try and fix the SQL using the LLM. \n",
    "You can simply specify this option when creating the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, use_query_checker=True)\n",
    "db_chain.run(\"How many albums by Aerosmith?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit query to number of rows\n",
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, use_query_checker=True, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "template1 = \"Give a summary of this employee's performance review:\\n{review}\"\n",
    "prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "chain_1 = LLMChain(llm=llm,\n",
    "                     prompt=prompt1,\n",
    "                     output_key=\"review_summary\")\n",
    "\n",
    "\n",
    "template2 = \"Identify key employee weaknesses in this review summary:\\n{review_summary}\"\n",
    "prompt2 = ChatPromptTemplate.from_template(template2)\n",
    "chain_2 = LLMChain(llm=llm,\n",
    "                     prompt=prompt2,\n",
    "                     output_key=\"weaknesses\")\n",
    "\n",
    "\n",
    "template3 = \"Create a personalized plan to help address and fix these weaknesses:\\n{weaknesses}\"\n",
    "prompt3 = ChatPromptTemplate.from_template(template3)\n",
    "chain_3 = LLMChain(llm=llm,\n",
    "                     prompt=prompt3,\n",
    "                     output_key=\"final_plan\")\n",
    "\n",
    "\n",
    "seq_chain = SequentialChain(chains=[chain_1,chain_2,chain_3],\n",
    "                            input_variables=['review'],\n",
    "                            output_variables=['review_summary','weaknesses','final_plan'],\n",
    "                            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_review = '''\n",
    "Employee Information:\n",
    "Name: Joe Schmo\n",
    "Position: Software Engineer\n",
    "Date of Review: July 14, 2023\n",
    "\n",
    "Strengths:\n",
    "Joe is a highly skilled software engineer with a deep understanding of programming languages, algorithms, and software development best practices. His technical expertise shines through in his ability to efficiently solve complex problems and deliver high-quality code.\n",
    "One of Joe's greatest strengths is his collaborative nature. He actively engages with cross-functional teams, contributing valuable insights and seeking input from others. His open-mindedness and willingness to learn from colleagues make him a true team player.\n",
    "Joe consistently demonstrates initiative and self-motivation. He takes the lead in seeking out new projects and challenges, and his proactive attitude has led to significant improvements in existing processes and systems. His dedication to self-improvement and growth is commendable.\n",
    "Another notable strength is Joe's adaptability. He has shown great flexibility in handling changing project requirements and learning new technologies. This adaptability allows him to seamlessly transition between different projects and tasks, making him a valuable asset to the team.\n",
    "Joe's problem-solving skills are exceptional. He approaches issues with a logical mindset and consistently finds effective solutions, often thinking outside the box. His ability to break down complex problems into manageable parts is key to his success in resolving issues efficiently.\n",
    "Weaknesses\n",
    "While Joe possesses numerous strengths, there are a few areas where he could benefit from improvement. One such area is time management. Occasionally, Joe struggles with effectively managing his time, resulting in missed deadlines or the need for additional support to complete tasks on time. Developing better prioritization and time management techniques would greatly enhance his efficiency.\n",
    "Another area for improvement is Joe's written communication skills. While he communicates well verbally, there have been instances where his written documentation lacked clarity, leading to confusion among team members. Focusing on enhancing his written communication abilities will help him effectively convey ideas and instructions.\n",
    "Additionally, Joe tends to take on too many responsibilities and hesitates to delegate tasks to others. This can result in an excessive workload and potential burnout. Encouraging him to delegate tasks appropriately will not only alleviate his own workload but also foster a more balanced and productive team environment.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = seq_chain(employee_review)\n",
    "results #dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['final_plan']) #index into dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Router chain\n",
    "use case example: route the prompt to most appropriate LLM chain. \n",
    "eg. internal employees vs external clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginner_template = '''You are a physics teacher who is really\n",
    "focused on beginners and explaining complex topics in simple to understand terms. \n",
    "You assume no prior knowledge. Here is the question\\n{input}'''\n",
    "\n",
    "expert_template = '''You are a world expert physics professor who explains physics topics\n",
    "to advanced audience members. You can assume anyone you answer has a \n",
    "PhD level understanding of Physics. Here is the question\\n{input}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#route prompt info\n",
    "#list of dict of name, description, template\n",
    "\n",
    "prompt_infos = [\n",
    "    {'name':'advanced physics',\n",
    "    'description': 'Answers advanced physics questions',\n",
    "     'prompt_template':expert_template},\\\n",
    "    \\\n",
    "    {'name':'beginner physics',\n",
    "    'description': 'Answers basic beginner physics questions',\n",
    "     'prompt_template':beginner_template},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict that refer to llm\n",
    "destination_chains = {}\n",
    "\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    #add to dict\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm = llm, \n",
    "                        prompt = default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "# print(MULTI_PROMPT_ROUTER_TEMPLATE)\n",
    "# print to see more info and hints\n",
    "\n",
    "# li comprehension for route destination\n",
    "# for every prompt info, grab the key name: value description \n",
    "# output is li of key:value\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "\n",
    "#join them to create a single string\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "\n",
    "# now you should see an output similar to print(MULTI_PROMPT_ROUTER_TEMPLATE)\n",
    "# but your template is in\n",
    "# print(router_template)\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )\n",
    "\n",
    "\n",
    "chain.run(\"How do magnets work?\")\n",
    "chain.run(\"How do Feynman Diagrams work?\")                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform chain\n",
    "- save some money by transforming yourself instead of asking llm to transform\n",
    "- uses a seq chain to chain a transform func and an instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_review ='''\n",
    "TITLE: AN ABSOLUTE DELIGHT! A CULINARY HAVEN!\n",
    "REVIEW:\n",
    "OH MY GOODNESS, WHERE DO I BEGIN? THIS RESTAURANT IS ABSOLUTELY PHENOMENAL! I WENT THERE LAST NIGHT WITH MY FRIENDS, AND WE WERE BLOWN AWAY BY THE EXPERIENCE!\n",
    "FIRST OF ALL, THE AMBIANCE IS OUT OF THIS WORLD! THE MOMENT YOU STEP INSIDE, YOU'RE GREETED WITH A WARM AND INVITING ATMOSPHERE. THE DECOR IS STUNNING, AND IT IMMEDIATELY SETS THE TONE FOR AN UNFORGETTABLE DINING EXPERIENCE.\n",
    "NOW, LET'S TALK ABOUT THE FOOD! WOW, JUST WOW! THE MENU IS A PARADISE FOR FOOD LOVERS. EVERY DISH WE ORDERED WAS A MASTERPIECE. THE FLAVORS WERE BOLD, VIBRANT, AND EXPLODED IN OUR MOUTHS. FROM STARTERS TO DESSERTS, EVERY BITE WAS PURE BLISS!\n",
    "THEIR SEAFOOD PLATTER IS A MUST-TRY! THE FRESHNESS OF THE SEAFOOD IS UNMATCHED, AND THE PRESENTATION IS SIMPLY STUNNING. I HAVE NEVER TASTED SUCH DELICIOUS AND PERFECTLY COOKED SEAFOOD IN MY LIFE. IT'S A SEAFOOD LOVER'S DREAM COME TRUE!\n",
    "THE SERVICE WAS EXEMPLARY. THE STAFF WAS ATTENTIVE, FRIENDLY, AND EXTREMELY KNOWLEDGEABLE ABOUT THE MENU. THEY WENT ABOVE AND BEYOND TO ENSURE THAT WE HAD THE BEST DINING EXPERIENCE POSSIBLE.\n",
    "AND LET'S NOT FORGET ABOUT THE DESSERTS! OH MY, OH MY! I HAD THEIR SIGNATURE CHOCOLATE LAVA CAKE, AND IT WAS PURE HEAVEN. THE CAKE WAS MOIST, AND WHEN I CUT INTO IT, THE WARM CHOCOLATE OOOZED OUT, CREATING AN EXPLOSION OF FLAVOR IN MY MOUTH. IT WAS LIKE A SYMPHONY OF CHOCOLATEY GOODNESS!\n",
    "IN CONCLUSION, THIS RESTAURANT IS A HIDDEN GEM! IF YOU WANT TO INDULGE IN A MEMORABLE DINING EXPERIENCE, DO YOURSELF A FAVOR AND VISIT THIS PLACE. YOU WON'T REGRET IT! I CAN'T WAIT TO GO BACK AND TRY MORE OF THEIR DELECTABLE DISHES. KUDOS TO THE ENTIRE TEAM FOR CREATING SUCH A CULINARY HAVEN!\n",
    "ALL I CAN SAY IS... WOOHOO!\n",
    "'''\n",
    "\n",
    "print(yelp_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(inputs: dict) -> dict = dict input dict output\n",
    "def transformer_fun(inputs: dict) -> dict:\n",
    "    '''\n",
    "    Notice how this always takes an inputs dictionary.\n",
    "    Also outputs a dictionary. You can call the output and input keys whatever you want, \n",
    "    just make sure to reference it correct in the chain call.\n",
    "    '''\n",
    "    # GRAB INCOMING CHAIN TEXT\n",
    "    text = inputs['text']\n",
    "\n",
    "    #grab review section only\n",
    "    only_review_text = text.split('REVIEW:')[-1]\n",
    "    \n",
    "    #lower case it\n",
    "    lower_case_text = only_review_text.lower()\n",
    "    \n",
    "    #return a dict\n",
    "    return {'output':lower_case_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_chain = TransformChain(input_variables=['text'],\n",
    "                                 output_variables=['output'],\n",
    "                                 transform=transformer_fun)\n",
    "\n",
    "\n",
    "template = \"Create a one sentence summary of this review:\\n{review_text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "summary_chain = LLMChain(llm=llm,\n",
    "                     prompt=prompt,\n",
    "                     output_key=\"review_summary\")\n",
    "\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain,summary_chain],\n",
    "                                        verbose=True)\n",
    "\n",
    "\n",
    "result = sequential_chain(yelp_review)\n",
    "\n",
    "print(result['output'])\n",
    "print(result['input'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
